{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRAW Analysis Procedure\n",
    "by Matthew Man\n",
    "\n",
    "20 Aug. 2019\n",
    "\n",
    "This document will detail the procedure of my analysis of the STRAW data. The results from this analysis are the optical properties of the medium at the time and wavelength of a run (a run refers to a single hdf5 file). Specifically, the attenuation length, absorption length, scattering length, and eta the fraction of molecular scattering, along with respective uncertainties, will be determined. From this, calculating other properties, for example the average cosine of scattering and the effective scattering length, is trivial.\n",
    "\n",
    "Note: My simulations are wrt POCAM2 only, so any distance calculation will need to be modified to handle POCAM1.\n",
    "\n",
    "The actual code is in various python files/Jupyter notebooks, where you can also see the output to each cell. Follow this tutorial, with reference to the cource code, to reproduce the analysis.\n",
    "\n",
    "My github repo https://github.com/matt457/STRAW_analysis\n",
    "\n",
    "STRAW data can be downloaded at http://tuphecp-haystack.ph.tum.de:10080/ftp/\n",
    "\n",
    "This document will cover: \n",
    "\t1. How the photon arrival time distribution is obtained from the data in the hdf5 file\n",
    "\t2. How to calculate (effective) attenuation length from measured time distributions\n",
    "\t3. How to simulate arrival time distributions\n",
    "\t4. How to fit simulations to the data to determine other optical properties\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Measured arrival time distribution\n",
    "\n",
    "For a complete explanation of this section, Akanksha Katil's documentation should be read because the code for this was originally written by her. Nonetheless the procedure will be documented here. Note that this section will often be quite manual. Unless otherwise specified, the inputs used here should apply in most cases and should not be altered. For reference, this process is also shown in https://github.com/matt457/STRAW_analysis/blob/master/P2%20Violet.ipynb\n",
    "\n",
    "The clean, run, and residual classes are defined in:\n",
    "\n",
    "https://github.com/matt457/STRAW_analysis/blob/master/clean.py\n",
    "\n",
    "https://github.com/matt457/STRAW_analysis/blob/master/run.py\n",
    "\n",
    "https://github.com/matt457/STRAW_analysis/blob/master/residual.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: There are values hardcoded into the files. In clean.py the file_path is hardcoded in to the init method. In residual.py the save_path is hardcoded in the init method, as well as the approximate POCAM interval in ns in the methods init, GetGausPeak, and minimizer. For POCAM1 5000Hz use 200100.71, for POCAM1 2500Hz use 400100.71, for POCAM2 5000Hz use 200101.33, for POCAM2 2500Hz use 400101.33 \n",
    "\n",
    "It may be worth determining the approximate interval automatically by checking the POCAM frequency in the variable 'values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean, run, residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize - input the file name\n",
    "filename = '20190426_085558_UTC_SDOM1_FLASH_POSEIDON1_P2_violet_both_2500Hz_20V_60s_19116085608.hld_up.hdf5'\n",
    "a = clean.clean(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of clean(...) not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "(abs_elim_3, rising_0_elim_3, rising_1_elim_3, rising_2_elim_3, rising_3_elim_3,\n",
    " falling_0_elim_3, falling_1_elim_3, falling_2_elim_3, falling_3_elim_3, POCAM_num, values,\n",
    "atstamp, p_jumps, dt_mean, f_r, file_path, SDOM_num, PMT,sub_time_elim_3,sub_id_elim_3) = a.P_S_used()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of P_S_used(...) not important, but interesting to visualize times and rates of events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize residual class\n",
    "# Note that inputs are dependent on POCAM frequency\n",
    "# Exact values do not matter\n",
    "\n",
    "r1 = residual.residual(abs_elim_3,rising_1_elim_3,\n",
    "                       400080, 400120, # Use POCAM interval +/- 20\n",
    "                       100,\n",
    "                       400095, 400105, # Use POCAM interval +/- 5\n",
    "                       0.0e10,0.6e10,0, 400000, # Optional: specifying the x and y window to visualize residuals\n",
    "                       file_path, values, POCAM_num, rising_0_elim_3,falling_0_elim_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important output of residual(...) is the first graph produced, a histogram of time differences between events that have an (approximately) gaussian distribution about the actual POCAM frequency. These are referred to as events_in_peak\n",
    "\n",
    "NOTE: this histogram can be empty, that's not a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) i) Visualizing residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D histogram of density of residuals over time\n",
    "gaus_peak = 400100.71 # approximate POCAM interval\n",
    "\n",
    "# first input is number of bins in histogram, decrease anywhere down to 300 for fainter signals\n",
    "abs_elim, BinsHist, JumpIndex, xedges, yedges, POCAM_bins, POCAM_diff = r1.HIST2D(500, gaus_peak, SDOM_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals are defined as time after POCAM pulse is emitted before it's detected.\n",
    "Calculated as: residual = (absolute_timestamp)%(POCAM_interval)\n",
    "\n",
    "The POCAM signal should be bright horizontal lines on the histograms. Take note of roughly where discontinuities in the residuals occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) ii) Exact POCAM frequency - Method 1\n",
    "Use when events_in_peak looks reasonably gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input absolute time range with no jumps from HIST2D(...)\n",
    "gaus_peak = r1.minimizer(0.0e10, 0.5e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) iii) Exact POCAM frequency - Method 2\n",
    "Will work any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input absolute time range with no jumps from HIST2D(...)\n",
    "gaus_peak = r3.GetGausPeak([4.5e10,5.9e10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) iv) Arrival time distribution - Method 1\n",
    "This method is easiest and works fine when there are many points in events_in_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs shouldn't need to be modified, and are hard to explain anyway\n",
    "t_res_all1,  num_events1, noise_events1 = r1.res(gaus_peak, 250, 5, bin_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output saved as csv and png. Arbitrarily determined, arrival times should be plotted from -100ns to 200 or 400ns, with the peak always at 0. Currently the analysis depends on 1ns spacing, but this can technically be varied.\n",
    "\n",
    "Important outputs of res(...) are the last two plots. The 2nd last is the arrival time distribution, and the last one is the recentered time residual of each slice in absolute time, which should line up around 0 with a little variance (black line is mean). If either of these look wrong, try method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) v) Arrival time distribution - Method 2\n",
    "Use this method when events_in_peak is sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun HIST2D(...) with exact POCAM interval\n",
    "abs_elim, BinsHist, JumpIndex, xedges, yedges, POCAM_bins, POCAM_diff = r1.HIST2D(500, gaus_peak, SDOM_num)\n",
    "\n",
    "# check that correct bins are found (aligned with signal, ie. bright horizontal lines), some error is fine\n",
    "plt.plot(POCAM_bins, '.')\n",
    "\n",
    "# Plot arrival time distribution, in small time window, and large one\n",
    "t_res_all1,num_events1, noise_events1 = r1.calc_res(BinsHist, 10, 2, 1,bin_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output (small time window) saved as csv and png\n",
    "\n",
    "If POCAM_bins are very inaccurate, or if arrival time distribution in small window looks poor, or if the time distr. in large window has multiple (significant) peaks, then try method 3. Generally method 2 only fails when the signal is very faint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) vi) Arrival time distribution - Method 3\n",
    "Use this method when the signal is very faint and all else fails. This method stitches together the jumps in residuals by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun HIST2D(...) with exact POCAM interval. Refer to the 2D histograms for the later steps\n",
    "abs_elim, BinsHist, JumpIndex, xedges, yedges, POCAM_bins, POCAM_diff = r1.HIST2D(500, gaus_peak, SDOM_num)\n",
    "\n",
    "# select time windows with no jumps (excluding some data is fine)\n",
    "time_window1 = (abs_elim_3>0.0e10) & (abs_elim_3<1.6e10)\n",
    "time_window2 = (abs_elim_3>4.7e10) & (abs_elim_3<5.0e10)\n",
    "time_window3 = abs_elim_3>5.7e10\n",
    "\n",
    "my_data_all = np.array([])\n",
    "weights_all = np.array([])\n",
    "\n",
    "# for a single window at a time,\n",
    "#  subtract from my_res and reduce plot range until peak is at time 0 with range=(-100,200)\n",
    "my_slice = (abs_elim_3+rising_1_elim_3)[time_window1] \n",
    "weights = r4.weights[time_window1]\n",
    "my_res = (my_slice%gaus_peak) - 223260\n",
    "my_data_all = np.append(my_data_all, my_res) # comment out when considering another window\n",
    "weights_all = np.append(weights_all,weights) # comment out when considering another window\n",
    "\n",
    "my_slice = (abs_elim_3+rising_1_elim_3)[time_window2]\n",
    "weights = r4.weights[time_window2]\n",
    "my_res = (my_slice%gaus_peak) #- 223260-100000+89-500-15\n",
    "#my_data_all = np.append(my_data_all, my_res)\n",
    "#weights_all = np.append(weights_all,weights)\n",
    "\n",
    "my_slice = (abs_elim_3+rising_1_elim_3)[time_window3]\n",
    "weights = r4.weights[time_window3]\n",
    "my_res = (my_slice%gaus_peak) #- 223260-100000+89-500-15-123\n",
    "#my_data_all = np.append(my_data_all, my_res)\n",
    "#weights_all = np.append(weights_all,weights)\n",
    "\n",
    "# plot arrival time distribution\n",
    "fig, ax = plt.subplots(figsize=(10,9))\n",
    "# vary range to best see peak. eg. range=(-10000,20000) for a rough search, range=(-100,200) for fine search\n",
    "n,bins,patches = ax.hist(my_data_all,bins=300,log=True, weights=weights_all, range=(-10000,20000))\n",
    "ax.axvline(color='k')\n",
    "\n",
    "# save to csv\n",
    "path = 'Data/POSEIDON1/Measured_arrival_times/'\n",
    "filename = \"['P2'],['SDOM3'],up,violet,['20V'],['2500Hz'].csv\"\n",
    "#with open(path+filename, 'w') as csvfile:\n",
    "#    writer = csv.writer(csvfile)\n",
    "#    writer.writerow(bins[:-1])\n",
    "#    writer.writerow(n)\n",
    "#csvfile.close()\n",
    "\n",
    "time_correction_4 = (abs_elim_3[-1]-abs_elim_3[0])/(1.6e10 + (5.0-4.7)*1e10 + (abs_elim_3[-1]-5.7e10))\n",
    "print(time_correction_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reference to the 2D histograms, select time windows with no jumps (excluding some data is fine). Select as many windows as necessary.\n",
    "\n",
    "For a single window at a time (comment out other append statements), start with a large plot range, then run the cell, then subtract the peak position from my_res and reduce plot range, then rerun the cell and continue to subtract from my_res to recenter peak to time 0 with final range=(-100,200).\n",
    "\n",
    "Alternatively, there's a much smarter way of doing this that I've yet to implement. Determine the peak position (median) from a histogram of each my_res slice with an increased number of bins (~80000, we want roughly 1ns bins), and subtract this peak from each my_res slice. Repeat for each slice. It's less manual but either way you still need to define slice ranges (time_window) and recenter the peaks by hand.\n",
    "\n",
    "When all windows have individually been recentered, uncomment all append statements to plot all slices together, with all peaks at 0.\n",
    "\n",
    "In some cases, the signal as seen from the 2D histograms is still too faint to discern above noise. In those cases there is nothing else you can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDOM1_POSEIDON_run = run.run()\n",
    "run_time, err_run_time, dead_time_uncert = sDOM1_POSEIDON_run.run_time(atstamp, p_jumps, dt_mean, a.f_0,\n",
    "                                                    a.f_1-a.r_1,\n",
    "                                                    a.f_2-a.r_2,\n",
    "                                                    a.f_3-a.r_3) # reminder: a is instance of the clean class\n",
    "\n",
    "print('dead_time_uncert: ', dead_time_uncert, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important outputs are run_time and dead_time_uncert. Currently time corrections are calculated to convert actual run_time to the expected run_time. For example:\n",
    "\n",
    "actual_run_time = 59s\n",
    "\n",
    "expected_run_time = 60s\n",
    "\n",
    "time_correction = 60/59 = 1.016949152542373\n",
    "\n",
    "Thus when counting photons, counts represents all hits integrated over time span specified by the run, and to compare between runs one must account for both POCAM frequency and run time. Another way of doing this is determining counts/sec.\n",
    "\n",
    "If method 3 was used, then run_time is defined as the sum of time considered in each time_window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Attenuation Length\n",
    "\n",
    "Refer to https://github.com/matt457/STRAW_analysis/blob/master/Attenuation%20Length.ipynb\n",
    "\n",
    "Run cells to perform imports, define fucntions photon_counter(...) and function attenuation_length(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data and to save files\n",
    "path_meas = \"Data/POSEIDON1/violet/Measured_arrival_times/\"\n",
    "path_save = \"Data/POSEIDON1/violet/Optimization/\"\n",
    "\n",
    "# \n",
    "time_correction = np.array([1.0144305079415528, 0.9885799387876817, 0.57504120677826, 0.3756827648418125])\n",
    "dead_time_uncert = np.array([0, 0.42392268563089913, 0.7531461586286848, 1.5592730733224076])\n",
    "\n",
    "peak_photons, total_photons, peak_uncertainty, total_uncertainty = photon_counter(path_meas, time_correction, dead_time_uncert)\n",
    "att_len, eff_att_len = attenuation_length(path_save, peak_photons, total_photons, peak_uncertainty, total_uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is data lists for photon counts with percent uncertainties, plots of the linearized intensity-distance relationship, with slope=-1/att_len. The attenuation length and effective attenuation length  with respective systematic uncertainties is calculated. The y-intercept is proportional to ln(N_tot) where N_tot is the total photons emitted. Photon is counted in peak if it lies within (-10,10) of arrival time distribution. \n",
    "\n",
    "By default, sDOM5 is excluded due to errant arrival time distribution shapes that give low photon counts. The reason is believed to be saturation of the PMT at short distances from the POCAM. \n",
    "\n",
    "Running photon_counter(...) also saves the arrival time distributions with corrections applied, which is used in fitting with simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simulations\n",
    "Refer to https://github.com/matt457/STRAW_analysis/blob/master/Photon%20MC%20simulation.ipynb\n",
    "\n",
    "Run cells sequentially to perform imports, define probability distributions and helper functions.\n",
    "\n",
    "As mentioned previously, simulations are wrt POCAM2. It is possible to simulate any distance, but the code must be modified for this (should be trivial to modify)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Running a single simulation\n",
    "This step is here for reference, and is not strictly necessary to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over photon_propagation and bin arrival times\n",
    "# Input: \n",
    "#   number of photons to simulate,\n",
    "#   absorption and scattering lengths in m, \n",
    "#   eta the fraction of molecular scattering (float: 0-1), \n",
    "#   colour of light (string: 'v'/'violet', 'b'/'blue', 'u'/'uv'),\n",
    "#   sDOM number (integer: 1-5), \n",
    "#   verbosity flag; when False suppress figures,\n",
    "#   bin width in ns,\n",
    "#   skip files already in path when True,\n",
    "#   save results when True.\n",
    "# Output: photon arrival times (0-150ns, 1ns uniform spacing) and save data as csv\n",
    "\n",
    "simulation(num_photons, abs_len, scatt_len, eta, colour, sDOM, save_path, verbose=False, bin_size=1, skip_repeat=True, save_results=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output only the execution time, unless verbose=True, in which case also plot the arrival time distribution.\n",
    "\n",
    "Note: Direct photon time is subtracted from travel times to determine arrival time distribution. However, it is sometimes found that the peak is slightly after the direct photon time. This is accounted for later but ideally the peak should be shifted to 0 either in the function photon_propagation(...) or simulation(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Loop over parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop over parameters and generate popt_array\n",
    "\n",
    "path = 'Data/Simulation_v2/uv/'\n",
    "\n",
    "# Parameter selection\n",
    "num_photons = 100000\n",
    "colour = 'uv'\n",
    "sDOM = 5 #1\n",
    "bin_size = 1 # (ns)\n",
    "\n",
    "# loop over paramters\n",
    "scatt_len_list = np.arange(10,26,5)\n",
    "abs_len_list = np.arange(25,41,5)\n",
    "eta_list = np.arange(.10,0.36,.05)\n",
    "\n",
    "# store data\n",
    "popt_array = np.zeros((len(abs_len_list),len(eta_list),len(scatt_len_list),4))\n",
    "\n",
    "for i, scatt_len in enumerate(scatt_len_list):\n",
    "    for j, eta in enumerate(eta_list):\n",
    "        for k, abs_len in enumerate(abs_len_list):\n",
    "            arr_time = simulation(num_photons, abs_len, scatt_len, eta, colour, sDOM, path, verbose=False, skip_repeat=True)\n",
    "            arr_time = arr_time / np.max(arr_time) # normalize\n",
    "\n",
    "            ind = np.argmax(arr_time) # shift peak to 0\n",
    "            sim_data_slice = arr_time[ind:-10] # remove edge effects from convolution\n",
    "            x = np.arange(0,len(sim_data_slice)) * bin_size\n",
    "            p0 = [10, .02, 3, -.01] # initial parameter guess\n",
    "            popt, pcov = curve_fit(gaussian_double_exponential,x,sim_data_slice,p0)\n",
    "            popt_array[k,j,i,:] = popt[:]\n",
    "\n",
    "np.save(path+'popt_array_sDOM%i_(abs[25,40],eta[.10,.35],scatt[10,25],popt)'%(sDOM),popt_array)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating 100,000 photons usually takes about 1-3 minutes, but can take anywhere up to 10 minutes when scattering length is very short. Currently I do not reccomend increasing the bin_size, as later steps assume 1ns bin_sizes. All other parameters should be specified by the user, and the code will loop over simulations for every value in scatt_len_list, abs_len_list, eta_list, such that the total number of iterations will be the product of the lengths of these lists. \n",
    "\n",
    "Curve fitting with a gaussian-double-exponential is performed on each simulation, and the curve-fit parameters (fwhm, a, k1, k2) are saved in popt_array. The definition of popt_array shows the values along each axis. 5m spacing on scatt/abs length and .05 spacing on eta gives a good balance of simulation time and resolution, but this can be reduced for better resolution. Increasing num_photons will also improve precision of simulations, at the cost of simulation time. \n",
    "\n",
    "#### Troubleshooting:\n",
    "\n",
    "first run plt.semilogy(arr_time) to see the distribution.\n",
    "\n",
    "Depending on the parameter values, curve_fit may fail. It may be that initial guess p0 is too far from popt, and may need to be adjusted. Try p0 = [10, .002, 3, -.03] when there is little scattering (scatt_len > 25m), and p0 = [10, .02, 3, -.01] when there is a lot of scattering (scatt_len < 25m). If that still doesn't work then play around with p0 till it does. \n",
    "\n",
    "It could also be that not enough events occur in highly scattered bins, so arr_time is not smooth enough. The naive way around this is to rerun the simulation in hopes for a smoother curve (this will in fact often work). Otherwise you may have to increase the statistics (eg. try 200,000 photons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Visualize parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize parameter space\n",
    "## plot 1/chi-squared as function of scatt_len and eta, for each abs_len value\n",
    "\n",
    "colour = 'uv'\n",
    "sDOM = 5\n",
    "\n",
    "save_path = 'Data/Simulation_v2/%s/'%colour\n",
    "file_sim = 'popt_array_sDOM%i_(abs[25,40],eta[.10,.35],scatt[10,25],popt).npy'%sDOM\n",
    "popt_array = np.load(save_path+file_sim) #sDOM1\n",
    "\n",
    "scatt_len_list = np.arange(10,26,5)\n",
    "abs_len_list = np.arange(25,41,5)\n",
    "eta_list = np.arange(.10,0.36,.05)\n",
    "\n",
    "# Get measurement data\n",
    "path_meas = 'Data/HERA1/%s/Measured_arrival_times/'%colour\n",
    "file = \"['P2'],['SDOM%i'],up,%s,['20V'],['2500Hz'].csv\"%(sDOM,colour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full source code is deferred here, but important part, the parameter selection, is included. Ensure everything matches up, including the parameter range of popt_array that simulations were run at, the file name of popt_array, the measurement file name and the path etc.\n",
    "\n",
    "The output is a heatmap of 1/chi_sqr between simulation and measurement for eta vs. scatt_len, at each value of abs_len. Printouts of min(chi_sqr) for each abs_len slice are given. Absolute chi_sqr values here are completely meaningless because the simulations are normalized differently than when the minimization is done, but relative values are still important.\n",
    "\n",
    "The take-away is that this section should be run to ensure the optimum fit lies somewhere within popt_array (and preferably not at any boundary). Else rerun popt_array at a different range. This visualization will also give a good value to seed the minimizer with. Also best to check each baseline that will be fit over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fit Simulations to Data\n",
    "Still with reference to https://github.com/matt457/STRAW_analysis/blob/master/Photon%20MC%20simulation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Cost function to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poissonian likelihood ratio\n",
    "def likelihood_ratio(f_obs, f_exp):\n",
    "    cost = 2*np.sum(f_exp - f_obs + f_obs*np.log(f_obs/f_exp))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Simultaneous fit\n",
    "Only half the function is shown here, where variables are set. Note that almost eveything shown here in the fit function is hardcoded (so inputs are only optical parameters to fit), and as such will need to be specified each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simultaneuous_fit_interpolated(abs_len, scatt_len, eta):\n",
    "    # Parameters\n",
    "    colour = 'uv'\n",
    "    save_path = 'Data/Simulation_v2/%s/'%colour\n",
    "    sDOM_list = [5,1] # specify baselines to fit over\n",
    "    scatt_len_list = np.arange(10,26,5) # must match parameters lists used in popt_array\n",
    "    abs_len_list = np.arange(25,41,5)\n",
    "    eta_list = np.arange(.10,0.36,.05)\n",
    "    #print(abs_len, scatt_len, eta) # for testing\n",
    "    \n",
    "    chi_sqr_total = 0\n",
    "    \n",
    "    for sDOM in sDOM_list:\n",
    "        # measurement\n",
    "        path_meas = 'Data/MINOS1/%s/Measured_arrival_times/'%colour \n",
    "        file = \"['P2'],['SDOM%i'],up,%s,['20V'],['2500Hz'],corrected.csv\"%(sDOM,colour) \n",
    "        my_data = np.genfromtxt(path_meas+file, delimiter=',')\n",
    "        times = my_data[0][100:(100+139)] # length is abitrary, but will vary with bin_size. \n",
    "        counts_slice = my_data[1][100:(100+139)]\n",
    "        \n",
    "        # simulation\n",
    "        file_sim = 'popt_array_sDOM%i_(abs[25,40],eta[.10,.35],scatt[10,25],popt).npy'%sDOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important notes on the fit function:\n",
    "\n",
    "A single set of simulations, at a specified baseline and wavelength, and represented by saved values in popt_array, can be used to fit to any run (over time) with the same wavelength and baseline. Temporal shifts shouldn't vary optical parameters greatly. \n",
    "\n",
    "\n",
    "times and counts_slice are sliced somewhat arbitrarily. 140ns appeared to be a good range in measurements, after which counts start dropping to a similar level to the background. Measurements start from -100ns with 1ns spacing, hence slicing according to [100:(100+139)]. This would need to change with bin_size. This also determines dofs of likelihood. eg. 139 bins * 2 baselines - 3 parameters - 1 = 274 degrees of freedom. \n",
    "\n",
    "\n",
    "Minimizer migrad tends to fail when the likelihood space is not smooth, which is true for simulations of 100,000 photons. Interpolation is the way around that, since linear interpolation will be smooth. The other fit functions simultaneuous_fit_2(...) run simulations at every function call, which is slow and noisy, but theoretically more accurate with larger statistics. Currently I think using simultaneuous_fit_interpolated is the best approach. Just be sure to visualize the parameter space before minimizing, otherwise it's hard to tell if results are sensible (eg. boundaries, local minima, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(simultaneuous_fit_interpolated, abs_len=30, scatt_len=15, eta=.15, # seed values\n",
    "           error_abs_len=3, error_scatt_len=2, error_eta=.02, \n",
    "           limit_abs_len=(25, 40), limit_scatt_len=(10, 25), limit_eta=(0.10, .35), # set limits\n",
    "           errordef=.5)\n",
    "\n",
    "\n",
    "print('Run optimiser')\n",
    "m.migrad()  # run optimiser\n",
    "pprint(m.get_param_states())\n",
    "print('fmin')\n",
    "pprint(m.get_fmin())\n",
    "\n",
    "print('Hesse errors')\n",
    "m.hesse()   # run covariance estimator\n",
    "print(m.errors)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameter limits according to those used in popt_array, otherwise interpolation will fail. Seed according to visualization of parameter space, ideally not on a boundary. The error_param values are initial step sizes (rule of thumb is 10% of seed value), and are not vital.\n",
    "\n",
    "Ensure the printout of fmin has is_valid=True, else results are meaningless. Also ensure that optimal parameters are not on a boundary, else rerun simulations with a wider range. Otherwise you have optimal parameter values and statistical uncertainties, and you're done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
